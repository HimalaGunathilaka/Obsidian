## ðŸ”— Linear Independence & Eigenvectors

A set of vectors is **linearly independent** if **none of them can be written as a linear combination of the others**.

### ðŸ§  Why the Definition Might Feel Backward

Mathematically, we define linear independence using:

$$
câ‚vâ‚ + câ‚‚vâ‚‚ + â‹¯ + câ‚™vâ‚™ = 0
$$

The vectors are **linearly independent** if the **only** solution is:

$$
câ‚ = câ‚‚ = â‹¯ = câ‚™ = 0
$$

That feels backward at first, but it's precise:  
If thereâ€™s **no nontrivial way** to combine them and get the zero vector, theyâ€™re independent.

---

## ðŸ’¡ Key Fact

> **Eigenvectors corresponding to distinct eigenvalues are linearly independent.**

---

### ðŸ“š Concepts Cheat Sheet

| Concept                     | Meaning                                                                        |
| --------------------------- | ------------------------------------------------------------------------------ |
| **Diagonalization**         | A matrix is diagonalizable if it has enough linearly independent eigenvectors. |
| **Jordan Form**             | Used when a matrix isnâ€™t diagonalizable.                                       |
| **Jordan Block**            | A block matrix with a repeated eigenvalue and 1s on the superdiagonal.         |
| **Generalized Eigenvector** | Helps form a complete basis when regular eigenvectors arenâ€™t enough.           |

---

## ðŸ“Œ What Is a Generalized Eigenvector?

Given a matrix **A** and eigenvalue **Î»**, a **generalized eigenvector** is a vector **v** such that:
$$
(A âˆ’ Î»I)^k v = 0 \quad \text{for some smallest integer } k > 1
$$

but:
$$
(A âˆ’ Î»I)^{kâˆ’1} v â‰  0
$$
So:
- Itâ€™s **not** a regular eigenvector (doesn't satisfy $(A - Î»I)v = 0$),
- But applying $(A - Î»I)$ **repeatedly** will eventually send it to zero.

---

## ðŸ§  The Kernel (Null Space)

The **kernel** (or **null space**) of a matrix **A** is the set of all vectors **x** such that:

$$A x = 0
$$

### âœ… Dimension of the Kernel

The **dimension** of the kernel is the number of **basis vectors** required to span it.

---

## âœ… Step-by-Step: Finding Eigenvectors

### Step 1: Set Up the Equation

You want to find non-zero vectors **v** such that:

$$
A v = Î» v
$$

Rewriting this:

$$
(A âˆ’ Î»I) v = 0
$$

This is a **homogeneous system** â€” you're looking for the **null space** of $(A - Î»I)$.

---
### Step 2: Solve the Null Space

1. Compute $(A - Î»I)$.
2. Solve:
$$
(A âˆ’ Î»I) v = 0
$$

This finds **all** eigenvectors corresponding to Î».

- The solution space may have infinitely many vectors.
- These vectors form a **basis** of the eigenspace.

---

## ðŸ” What Does "Similar Matrices" Mean?

> **Matrix A is similar to matrix B** if:
$$
A = P B P^{-1}
$$

for some **invertible matrix** $P$.

---

### ðŸ§© Algebraic vs Geometric Multiplicity

| Term                          | Meaning                                                                          |
|-------------------------------|----------------------------------------------------------------------------------|
| **Algebraic Multiplicity**    | How many times $$Î»$$ appears as a root of the **characteristic polynomial**     |
| **Geometric Multiplicity**    | The **dimension of the eigenspace**: the number of linearly independent eigenvectors |

- The **number of Jordan blocks** for Î» = **geometric multiplicity**.
- The **sum of block sizes** = **algebraic multiplicity**.

---

## ðŸŒ What Is the Image of a Matrix?

The **image** of a matrix $M$, written as $\text{Im}(M)$, is:

$$
\text{Im}(M) = \{ Mv \mid v \in \mathbb{R}^n \}
$$

Itâ€™s the set of all **output vectors** you can get by applying $M$ to some input.

---

### ðŸ§  Understanding Example:

Suppose weâ€™re analyzing:
$$
(A - I) m_3 = m_2
$$

This means:

- $m_2 \in \text{Im}(A - I)$
- There **exists** some $m_3$ such that $(A - I)m_3 = m_2$
---
# Hermitian
$A \in \mathbb{C}^{n \times n}$ is **Hermitian** iff  
$$
A^H = A,
$$  
where $A^H$ is the conjugate transpose of $A$, i.e., $A^H = \overline{A}^T$.
>[!theorem]
**Theorem 32.** Let $A \in \mathbb{C}^{n \times n}$ be Hermitian, then  
>1. The eigenvalues of $A$ are real.  
>2. Eigenvectors corresponding to distinct eigenvalues are orthogonal.  
>3. (**Spectral Theorem**) $A$ has $n$ orthogonal (i.e., orthonormal) eigenvectors.

- A matrix $P \in \mathbb{C}^{n \times n}$ is said to be **~={blue}Unitary=~** iff $P^H P = P P^H = I,$  i.e., when $P^{-1} = P^H$.  
- If $P$ has orthonormal columns, then $P$ is unitary.
- A matrix  is Positive Definite (PD) iff  $x^H A x > 0 \quad \text{for all } x \neq 0.$

>[!theorem]
>If $A$ is Hermitian, then $A$ is **positive definite** iff all its eigenvalues are **positive**.
